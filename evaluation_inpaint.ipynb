{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee43cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, AutoencoderKL\n",
    "import timm\n",
    "import lpips\n",
    "from helper import load_images_from_path, norm_imagenet, denorm_imagenet\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize_img,\n",
    "])\n",
    "\n",
    "def load_img(path, transforms=None):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = transforms(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "def norm_tensor(tensor):\n",
    "    t = tensor.clone().detach()\n",
    "    \n",
    "    min_val = t.min()\n",
    "    max_val = t.max()\n",
    "\n",
    "    tensor_norm = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    print(f\"Tensor normalized: min={tensor_norm.min()}, max={tensor_norm.max()}\")\n",
    "    \n",
    "    return tensor_norm, min_val, max_val\n",
    "\n",
    "def denorm_tensor(tensor, original_min=None, original_max=None):\n",
    "    t = tensor.clone().detach()\n",
    "\n",
    "    return t * (original_max - original_min) + original_min\n",
    "\n",
    "def create_random_mask(img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "    _, _, height, width = img_pt.shape\n",
    "    mask_area = int(height * width * mask_percentage)\n",
    "    masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "    if mask_percentage >= 0.999:\n",
    "        # Full mask for entire image\n",
    "        return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "    for ii in range(num_masks):\n",
    "        placed = False\n",
    "        attempts = 0\n",
    "        while not placed and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            max_dim = int(mask_area ** 0.5)\n",
    "            mask_width = random.randint(1, max_dim)\n",
    "            mask_height = mask_area // mask_width\n",
    "\n",
    "            # Allow broader aspect ratios for larger masks\n",
    "            aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "            if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                if mask_height <= height and mask_width <= width:\n",
    "                    x_start = random.randint(0, width - mask_width)\n",
    "                    y_start = random.randint(0, height - mask_height)\n",
    "                    overlap = False\n",
    "                    for jj in range(ii):\n",
    "                        if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                            overlap = True\n",
    "                            break\n",
    "                    if not overlap:\n",
    "                        masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                        placed = True\n",
    "\n",
    "        if not placed:\n",
    "            # Fallback: just fill a central region if all attempts fail\n",
    "            print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "            center_h = height // 2\n",
    "            center_w = width // 2\n",
    "            half_area = int((mask_area // 2) ** 0.5)\n",
    "            h_half = min(center_h, half_area)\n",
    "            w_half = min(center_w, half_area)\n",
    "            masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "    return masks.to(img_pt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ad4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"Hyperparameters and configuration settings for FreqLoc.\"\"\"\n",
    "    def __init__(self):\n",
    "        # --- System & Paths ---\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.train_datasets = '/mnt/nas5/suhyeon/datasets/valAGE-Set'\n",
    "        self.image_path = '/mnt/nas5/suhyeon/datasets/valAGE-Set/0088.png'\n",
    "        self.exp_name = 'baseline'\n",
    "        self.output_dir = f'/mnt/nas5/suhyeon/projects/freq-loc/{self.exp_name}'\n",
    "\n",
    "        # --- Model Configurations ---\n",
    "        self.vae_model_name = \"stabilityai/stable-diffusion-2-1\"\n",
    "        self.vae_subfolder = \"vae\"\n",
    "        \n",
    "        # --- Image Size Parameters ---\n",
    "        self.vae_image_size = 512\n",
    "        self.image_size = 256\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # --- FreqLoc Core Parameters ---\n",
    "        self.message_bits = 48\n",
    "        self.margin = 1.0\n",
    "        self.grid_size = 28\n",
    "        self.mask_percentage = 0.3\n",
    "        self.num_masks = 1\n",
    "        self.seed = 42\n",
    "        self.num_inference_steps = 100\n",
    "        self.guidance_scale = 7.5\n",
    "\n",
    "        # --- Optimization Parameters ---\n",
    "        self.lr = 2.0\n",
    "        self.steps = 500\n",
    "        self.lambda_p = 0.0025 #0.025\n",
    "        self.lambda_i = 0.005 #0.005\n",
    "        self.feat_layer = 1\n",
    "\n",
    "        # --- Robustness Parameters --- \n",
    "        self.eps0_std = [0.0, 0.8] # Latent noise\n",
    "        \n",
    "        # --- Demo/Evaluation Parameters ---\n",
    "        self.batch_size = 1\n",
    "        self.num_test_images = 1\n",
    "\n",
    "        self.feature_dim = None\n",
    "        if self.feat_layer == 0:\n",
    "            self.feature_dim = 96\n",
    "        elif self.feat_layer == 1:\n",
    "            self.feature_dim = 192\n",
    "        elif self.feat_layer == 2:\n",
    "            self.feature_dim = 384\n",
    "        elif self.feat_layer == 3:\n",
    "            self.feature_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ed1cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqLoc:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        # Initialize networks\n",
    "        self.pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "            \"sd-legacy/stable-diffusion-inpainting\",\n",
    "            # torch_dtype=torch.float16,\n",
    "            cache_dir='/mnt/nas5/suhyeon/caches'\n",
    "        ).to(self.args.device)\n",
    "        self.image_encoder = timm.create_model(\n",
    "            'convnext_small.dinov3_lvd1689m',\n",
    "            pretrained=True,\n",
    "            features_only=True\n",
    "        ).to(self.args.device)\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pipe.vae.requires_grad_(False)\n",
    "        self.pipe.unet.requires_grad_(False)\n",
    "        self.pipe.text_encoder.requires_grad_(False)\n",
    "        self.pipe.vae.eval()\n",
    "        self.pipe.unet.eval()\n",
    "        self.pipe.text_encoder.eval()\n",
    "        \n",
    "        # self.direction_vectors = torch.load('/mnt/nas5/suhyeon/projects/freq-loc/random_vec.pt').to(self.args.device)\n",
    "        self.direction_vectors = torch.load(f'/mnt/nas5/suhyeon/projects/freq-loc/random_vec_univ_{self.args.feature_dim}.pt').to(self.args.device)\n",
    "        # self.direction_vectors = self.generate_universal_vectors(self.args.feature_dim)\n",
    "        # torch.save(self.direction_vectors, f'/mnt/nas5/suhyeon/projects/freq-loc/random_vec_univ_{self.args.feature_dim}.pt')\n",
    "        self.num_patches = (self.args.image_size // 14) ** 2\n",
    "\n",
    "        self.loss_fn_vgg = lpips.LPIPS(net='alex').to(self.args.device)\n",
    "        self.loss_fn_vgg.eval()\n",
    "\n",
    "    def generate_universal_vectors(self, feature_dim):\n",
    "        \"\"\"\n",
    "        어떤 Feature가 들어와도 DC 성분(크기)을 무시하고 \n",
    "        방향만 검출할 수 있는 Universal Vector 생성\n",
    "        \"\"\"\n",
    "        # 1. 랜덤 생성\n",
    "        vecs = torch.randn(1, feature_dim)\n",
    "        \n",
    "        # 2. [핵심] Zero-Mean Centering (평균 제거)\n",
    "        # 각 벡터(row)의 평균을 계산해서 뺌 -> 합이 0이 됨\n",
    "        vecs = vecs - vecs.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # 3. Sign Quantization (강건성 향상)\n",
    "        # 0인 경우를 방지하기 위해 아주 작은 noise 추가 후 sign\n",
    "        vecs = torch.sign(vecs + 1e-6)\n",
    "        \n",
    "        # 4. L2 Normalization\n",
    "        vecs = vecs / torch.norm(vecs, p=2, dim=1, keepdim=True)\n",
    "        \n",
    "        return vecs.to(self.args.device)\n",
    "\n",
    "    def _create_random_mask(self, img_pt, num_masks=1, mask_percentage=0.1, max_attempts=100):\n",
    "        _, _, height, width = img_pt.shape\n",
    "        mask_area = int(height * width * mask_percentage)\n",
    "        masks = torch.zeros((num_masks, 1, height, width), dtype=img_pt.dtype)\n",
    "\n",
    "        if mask_percentage >= 0.999:\n",
    "            # Full mask for entire image\n",
    "            return torch.ones((num_masks, 1, height, width), dtype=img_pt.dtype).to(img_pt.device)\n",
    "\n",
    "        for ii in range(num_masks):\n",
    "            placed = False\n",
    "            attempts = 0\n",
    "            while not placed and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "\n",
    "                max_dim = int(mask_area ** 0.5)\n",
    "                mask_width = random.randint(1, max_dim)\n",
    "                mask_height = mask_area // mask_width\n",
    "\n",
    "                # Allow broader aspect ratios for larger masks\n",
    "                aspect_ratio = mask_width / mask_height if mask_height != 0 else 0\n",
    "                if 0.25 <= aspect_ratio <= 4:  # Looser ratio constraint\n",
    "                    if mask_height <= height and mask_width <= width:\n",
    "                        x_start = random.randint(0, width - mask_width)\n",
    "                        y_start = random.randint(0, height - mask_height)\n",
    "                        overlap = False\n",
    "                        for jj in range(ii):\n",
    "                            if torch.sum(masks[jj, :, y_start:y_start + mask_height, x_start:x_start + mask_width]) > 0:\n",
    "                                overlap = True\n",
    "                                break\n",
    "                        if not overlap:\n",
    "                            masks[ii, :, y_start:y_start + mask_height, x_start:x_start + mask_width] = 1\n",
    "                            placed = True\n",
    "\n",
    "            if not placed:\n",
    "                # Fallback: just fill a central region if all attempts fail\n",
    "                print(f\"Warning: Failed to place mask {ii}, using fallback.\")\n",
    "                center_h = height // 2\n",
    "                center_w = width // 2\n",
    "                half_area = int((mask_area // 2) ** 0.5)\n",
    "                h_half = min(center_h, half_area)\n",
    "                w_half = min(center_w, half_area)\n",
    "                masks[ii, :, center_h - h_half:center_h + h_half, center_w - w_half:center_w + w_half] = 1\n",
    "\n",
    "        return masks.to(img_pt.device)\n",
    "\n",
    "    def embed_watermark(self, original: torch.Tensor, img_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed watermark in image using latent frequency space optimization\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor [B, C, H, W]\n",
    "            message: Binary message {-1, 1} [B, message_bits]\n",
    "        \n",
    "        Returns:\n",
    "            Watermarked image tensor\n",
    "        \"\"\"\n",
    "\n",
    "        original = original.to(self.args.device)\n",
    "        # message = message.to(self.device)\n",
    "        \n",
    "        # Step 1: Encode image to latent space\n",
    "        image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "        latent = self.pipe.vae.encode(2*image-1).latent_dist.sample() # [-1, 1], [B,4,64,64]\n",
    "        \n",
    "        # Step 2: Transform to frequency domain\n",
    "        # latent_fft = torch.fft.fft2(latent, dim=(-2, -1))\n",
    "        \n",
    "        # Step 3: Initialize perturbation (trainable parameter)\n",
    "        # delta_m = torch.zeros_like(latent_fft, requires_grad=True)\n",
    "        delta_m = torch.zeros_like(latent, requires_grad=True)\n",
    "        optimizer = optim.Adam([delta_m], lr=self.args.lr)\n",
    "\n",
    "        # input = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "        # adaptive_weight = self._get_feature_weight(input, min_weight=0.3)\n",
    "\n",
    "        # Training loop\n",
    "        for step in range(self.args.steps):\n",
    "        # for step in tqdm(range(self.args.steps), desc=\"Embedding Watermark\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mask = self._create_random_mask(image, num_masks=1, mask_percentage=self.args.mask_percentage)\n",
    "            mask = mask.to(self.args.device)\n",
    "            target_mask = mask * 2 - 1 # Convert to {-1, 1}\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                mask = 1 - mask\n",
    "\n",
    "            image = F.interpolate(original, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "            mask = F.interpolate(mask, size=(self.args.vae_image_size, self.args.vae_image_size), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            # perturbed_fft = latent_fft + delta_m\n",
    "            # perturbed_latent = torch.fft.ifft2(perturbed_fft, dim=(-2, -1)).real\n",
    "            perturbed_latent = latent + delta_m\n",
    "\n",
    "            watermarked_image = self.pipe.vae.decode(perturbed_latent).sample\n",
    "            watermarked_image = (watermarked_image + 1) / 2\n",
    "            \n",
    "            masked = watermarked_image * mask + (1 - mask) * image\n",
    "\n",
    "            # uniform noise\n",
    "            latent_mask = F.interpolate(mask, size=(64, 64), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            std_val_0 = random.uniform(self.args.eps0_std[0], self.args.eps0_std[1])\n",
    "            eps0 = torch.randn_like(perturbed_latent) * std_val_0\n",
    "\n",
    "            perturbed_latent_1 = (perturbed_latent + eps0)*latent_mask + perturbed_latent*(1-latent_mask)\n",
    "\n",
    "            watermarked_image_1 = self.pipe.vae.decode(perturbed_latent_1).sample\n",
    "            masked_1 = (watermarked_image_1 + 1) / 2\n",
    "            masked_1 = masked_1 * mask + (1 - mask) * image\n",
    "\n",
    "            # Compute losses\n",
    "            image = F.interpolate(original, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            mask = F.interpolate(mask, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            target_mask = F.interpolate(target_mask, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            masked = F.interpolate(masked, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            masked_1 = F.interpolate(masked_1, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            watermarked_image = F.interpolate(watermarked_image, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            watermarked_image_1 = F.interpolate(watermarked_image_1, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            watermarked_image = norm_imagenet(watermarked_image)\n",
    "            masked = norm_imagenet(masked)\n",
    "            masked_1 = norm_imagenet(masked_1)\n",
    "\n",
    "            epsilon = 1e-6\n",
    "\n",
    "            features = self.image_encoder(watermarked_image)[self.args.feat_layer]\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            features_norm = features / (torch.norm(features, p=2, dim=-1, keepdim=True) + epsilon)\n",
    "            cosine_similarity = torch.matmul(features_norm, self.direction_vectors.T)\n",
    "\n",
    "            features = self.image_encoder(watermarked_image_1)[self.args.feat_layer]\n",
    "            features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            features_norm = features / (torch.norm(features, p=2, dim=-1, keepdim=True) + epsilon)\n",
    "            cosine_similarity_1 = torch.matmul(features_norm, self.direction_vectors.T)\n",
    "\n",
    "            B = cosine_similarity.shape[0]\n",
    "            H = W = int(cosine_similarity.shape[1] ** 0.5)\n",
    "\n",
    "            target_cosine = 0.15 # 1.0 - 1.5\n",
    "            loss_m = torch.mean(F.relu(target_cosine - cosine_similarity))\n",
    "            loss_m1 = torch.mean(F.relu(target_cosine - cosine_similarity_1))\n",
    "\n",
    "            loss_f = self._dice_loss(cosine_similarity, mask)\n",
    "            loss_f1 = self._dice_loss(cosine_similarity_1, mask)\n",
    "\n",
    "            watermarked_image = denorm_imagenet(watermarked_image)\n",
    "            masked = denorm_imagenet(masked)\n",
    "            masked_1 = denorm_imagenet(masked_1)\n",
    "\n",
    "            loss_psnr = self._psnr_loss(watermarked_image, image)\n",
    "            loss_lpips = self._lpips_loss(watermarked_image, image)\n",
    "\n",
    "            clean_weight = 1.0\n",
    "            noisy_weight = 1.0\n",
    "            \n",
    "            total_loss = clean_weight * (loss_m) + \\\n",
    "                         noisy_weight * (loss_m1) + \\\n",
    "                         0.2 * loss_f + 0.2 * loss_f1 + \\\n",
    "                         self.args.lambda_p * loss_psnr + \\\n",
    "                         self.args.lambda_i * loss_lpips\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step == 0 or (step+1) % 100 == 0:\n",
    "                psnr_val = self._compute_psnr(watermarked_image.detach(), image.detach())\n",
    "                print(f\"Step {step+1}, Loss: {total_loss.item():.4f}, PSNR: {psnr_val:.2f}\")\n",
    "                print(f\"Mask Loss: {loss_m.item():.4f}\") #, DICE Loss: {loss_d.item():.4f}\")\n",
    "                print(f\"Mask1 Loss: {(loss_m1).item():.4f}\") #, DICE1 Loss: {loss_d1.item():.4f}\")\n",
    "                print(f\"Focal Loss: {loss_f.item():.4f}, Focal1 Loss: {loss_f1.item():.4f}\")\n",
    "                print(f\"PSNR Loss: {loss_psnr.item():.4f}, LPIPS Loss: {loss_lpips.item():.4f}\")\n",
    "\n",
    "        # Final watermarked image\n",
    "        # wm_fft = latent_fft + delta_m\n",
    "        # wm_latent = torch.fft.ifft2(wm_fft, dim=(-2, -1)).real\n",
    "        wm_latent = latent + delta_m\n",
    "        rec_wm = self.pipe.vae.decode(wm_latent).sample\n",
    "        rec_wm = (rec_wm + 1) / 2\n",
    "\n",
    "        rec_clean = self.pipe.vae.decode(latent).sample\n",
    "        rec_clean = (rec_clean + 1) / 2\n",
    "\n",
    "        pixel_delta = rec_wm - rec_clean\n",
    "\n",
    "        final_images = torch.clamp(rec_clean + 1.0 * pixel_delta, 0, 1)\n",
    "        \n",
    "        return final_images.detach(), pixel_delta.detach()\n",
    "        \n",
    "    def decode_watermark(self, watermarked_image: torch.Tensor) -> torch.Tensor:\n",
    "        watermarked_image = watermarked_image.to(self.args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            watermarked_image = norm_imagenet(watermarked_image) \n",
    "            features = self.image_encoder(watermarked_image)[self.args.feat_layer]\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "            # dot_products = torch.matmul(features, self.direction_vectors.T) # [1, 256, 384]*[1, 384, 256] -> [1, 256, 1]\n",
    "\n",
    "            epsilon = 1e-6\n",
    "            features_norm = features / (torch.norm(features, p=2, dim=-1, keepdim=True) + epsilon)\n",
    "            direction_norm = self.direction_vectors / (torch.norm(self.direction_vectors, p=2, dim=-1, keepdim=True) + epsilon)\n",
    "            dot_products = torch.matmul(features_norm, direction_norm.T)\n",
    "\n",
    "            B = dot_products.shape[0]\n",
    "            H = W = int(dot_products.shape[1] ** 0.5)\n",
    "            grid = dot_products.view(B, H, W).unsqueeze(0) # [1, 256, 1] -> [1, 1, 16, 16]\n",
    "            grid = F.interpolate(grid, size=self.args.image_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # threshold = 0.1\n",
    "            # binary_prediction = (grid >= threshold).float()\n",
    "            temperature = 5.0  # (5.0 ~ 10.0 사이의 값으로 실험 필요)\n",
    "            scaled_grid = grid * temperature\n",
    "            confidence_map = torch.sigmoid(scaled_grid)\n",
    "            binary_prediction = (confidence_map >= 0.5).float()\n",
    "\n",
    "        return binary_prediction\n",
    "    \n",
    "    def _psnr_loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Negative PSNR loss (Equation 5)\"\"\"\n",
    "        mse = F.mse_loss(pred, target)\n",
    "        psnr = -10 * torch.log10(mse + 1e-8)\n",
    "        return -psnr  # Negative for minimization\n",
    "    \n",
    "    def _lpips_loss(self, pred, target):\n",
    "        pred_norm = pred * 2 - 1 # [-1, 1]\n",
    "        target_norm = target * 2 - 1 # [-1, 1]\n",
    "        return self.loss_fn_vgg(pred_norm, target_norm).mean()\n",
    "    \n",
    "    def _compute_psnr(self, pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"Compute PSNR between images\"\"\"\n",
    "        mse = F.mse_loss(pred, target).item()\n",
    "        if mse == 0:\n",
    "            return 100.0\n",
    "        return 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "    \n",
    "    def _dice_loss(self, cos_sim, gt_mask, smooth=1e-5):\n",
    "        temperature = 5\n",
    "        B = cos_sim.shape[0]\n",
    "        H = W = int(cos_sim.shape[1] ** 0.5)\n",
    "        grid = cos_sim.view(B, H, W).unsqueeze(0)\n",
    "        grid = F.interpolate(grid, size=self.args.image_size, mode='bilinear', align_corners=False) # [B, Num_Patches, Feature_Dim]*[B, Feature_Dim, 1] = [B, Num_Patches, 1]\n",
    "        pred = torch.sigmoid(grid * temperature) # Logits to probabilities\n",
    "       \n",
    "        # Flatten label and prediction tensors\n",
    "        pred = pred.view(-1)\n",
    "        target = gt_mask.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice_coeff = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "        \n",
    "        return 1 - dice_coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50bf3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = F.mse_loss(a, b).item()\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(torch.tensor(mse)))\n",
    "\n",
    "def calculate_iou(pred_mask, gt_mask):\n",
    "    # Ensure masks are binary\n",
    "    # pred_mask_bin = (pred_mask < 0).float()\n",
    "    pred_mask_bin = torch.sigmoid(pred_mask)\n",
    "    pred_mask_bin = (pred_mask_bin > 0.65).float() # Thresholding at 0.65\n",
    "    gt_mask_bin = (gt_mask > 0).float() # Ground truth might not be 0/1\n",
    "\n",
    "    save_image(pred_mask, \"pred.png\")\n",
    "    save_image(pred_mask_bin, \"pred_bin.png\")\n",
    "    save_image(gt_mask_bin, \"gt.png\")\n",
    "    save_image(pred_mask_bin * gt_mask_bin, \"intersection.png\")\n",
    "    save_image(pred_mask_bin + gt_mask_bin, \"union.png\")\n",
    "\n",
    "    # Intersection and Union\n",
    "    intersection = (pred_mask_bin * gt_mask_bin).sum()\n",
    "    union = (pred_mask_bin + gt_mask_bin).sum() - intersection\n",
    "\n",
    "    iou = intersection / (union + 1e-6) # Add epsilon to avoid division by zero\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "173a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/secret_code/0002.png\"\n",
    "# img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/secret_code/analysis_dist_wm_step400.png\"\n",
    "img_path = \"/mnt/nas5/suhyeon/projects/freq-loc/baseline/20251120-152728/watermarked/0088.png\"\n",
    "seed = 45\n",
    "proportion_masked = 0.3\n",
    "trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7288e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  8.42it/s]An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.78it/s]\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  9.54it/s]An error occurred while trying to fetch /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /mnt/nas5/suhyeon/caches/models--sd-legacy--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/envs/stableguard/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 30.63, IoU: 0.8810\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 30.63, IoU: 0.8518\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 30.63, IoU: 0.8593\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 30.63, IoU: 0.8281\n",
      "Tensor normalized: min=0.0, max=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 30.63, IoU: 0.8928\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"sd-legacy/stable-diffusion-inpainting\",\n",
    "    # torch_dtype=torch.float16,\n",
    "    cache_dir='/mnt/nas5/suhyeon/caches'\n",
    ").to(device)\n",
    "\n",
    "args = Params()\n",
    "freqmark = FreqLoc(args=args)\n",
    "\n",
    "# secret_key = torch.load('./learned_directional_vector.pt')\n",
    "# freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "# print(freqmark.direction_vectors)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "watermarked = load_img(img_path, transforms=args.transform)\n",
    "original = load_img('/mnt/nas5/suhyeon/datasets/valAGE-Set/0088.png', transforms=val_transforms)\n",
    "\n",
    "original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "psnrs = []\n",
    "ious = []\n",
    "logits = []\n",
    "\n",
    "for _ in range(trials):\n",
    "    mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "    img_norm, min_norm, max_norm = norm_tensor(watermarked)\n",
    "    img_edit_pil = pipe(prompt=\"\", image=img_norm, mask_image=mask, generator=generator).images[0]\n",
    "    img_edit = to_tensor(img_edit_pil)\n",
    "    img_edit = img_edit.unsqueeze(0).to(device)\n",
    "\n",
    "    img_edit = denorm_tensor(img_edit, min_norm, max_norm)  # [1, 3, H, W]\n",
    "    # img_edit = img_edit * mask + watermarked * (1-mask)\n",
    "\n",
    "    img_edit = F.interpolate(img_edit, size=(args.image_size, args.image_size), mode=\"bilinear\", align_corners=False)\n",
    "    decoded_batch = freqmark.decode_watermark(img_edit)\n",
    "\n",
    "    save_image(img_edit, \"edited.png\")\n",
    "    original = F.interpolate(original, size=(args.image_size, args.image_size), mode=\"bilinear\", align_corners=False)\n",
    "    watermarked_224 = F.interpolate(watermarked, size=(args.image_size, args.image_size), mode=\"bilinear\", align_corners=False)\n",
    "    mask_224 = F.interpolate(mask, size=(args.image_size, args.image_size), mode=\"bilinear\", align_corners=False)\n",
    "    psnrs.append(compute_psnr(watermarked_224, original))\n",
    "    ious.append(calculate_iou(decoded_batch, 1-mask_224))\n",
    "    logits.append(decoded_batch)\n",
    " \n",
    "    print(f\"PSNR: {psnrs[-1]:.2f}, IoU: {ious[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6fe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = Params()\n",
    "# freqmark = FreqMark(args=args)\n",
    "\n",
    "# # secret_key = torch.load('./learned_directional_vector.pt')\n",
    "# # freqmark.direction_vectors = torch.tensor(secret_key).to(args.device)\n",
    "# # print(freqmark.direction_vectors)\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# generator = torch.Generator(device=device).manual_seed(seed)\n",
    "# to_tensor = transforms.ToTensor()\n",
    "\n",
    "# watermarked = load_img(img_path, transforms=args.transform)\n",
    "# original = load_img('/mnt/nas5/suhyeon/datasets/DIV2K_train_HR/0002.png', transforms=val_transforms)\n",
    "\n",
    "# original = F.interpolate(original, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "# watermarked = F.interpolate(watermarked, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# psnrs = []\n",
    "# ious = []\n",
    "# logits = []\n",
    "\n",
    "# for _ in range(trials):\n",
    "#     mask = create_random_mask(watermarked, num_masks=1, mask_percentage=proportion_masked)\n",
    "\n",
    "#     delta = torch.load('delta_m.pt').to(device)\n",
    "\n",
    "#     original_norm = torch.linalg.norm(delta)\n",
    "\n",
    "#     # 2. '구조 없는' 워터마크 생성\n",
    "#     delta_m_random = torch.randn_like(delta)\n",
    "#     random_norm = torch.linalg.norm(delta_m_random)\n",
    "#     delta_m_random = delta_m_random * (original_norm / random_norm) # 세기를 동일하게 맞춤\n",
    "\n",
    "#     # 3. '세기만 약한' 워터마크 생성\n",
    "#     delta_m_scaled = delta * 0.5\n",
    "    \n",
    "#     results = {}\n",
    "\n",
    "#     latent = freqmark.vae.encode(2*original-1).latent_dist.sample()\n",
    "#     latent_fft = torch.fft.fft2(latent, dim=(-2, -1))\n",
    "\n",
    "#     for name, delta in [(\"Optimized\", delta), \n",
    "#                         (\"Random\", delta_m_random), \n",
    "#                         (\"Scaled\", delta_m_scaled)]:\n",
    "        \n",
    "#         final_fft = latent_fft + delta\n",
    "#         final_latent = torch.fft.ifft2(final_fft, dim=(-2, -1)).real\n",
    "#         watermarked_image = (freqmark.vae.decode(final_latent).sample + 1) / 2\n",
    "        \n",
    "#         watermarked_image = F.interpolate(watermarked_image, size=(args.dino_image_size, args.dino_image_size), mode=\"bilinear\", align_corners=False)\n",
    "#         logits = freqmark.decode_watermark(watermarked_image) \n",
    "        \n",
    "#         # 워터마크가 있어야 할 영역(gt_mask=1)의 평균 logit 점수 계산\n",
    "#         # avg_logit = (logits * gt_mask_resized).sum() / gt_mask_resized.sum()\n",
    "#         # results[name] = avg_logit.item()\n",
    "#         save_image(logits, f\"logits_{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c890fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Average on 5 trials ##\n",
      "PSNR (imperceptibility): 30.63 dB\n",
      "IoU (localization accuracy): 0.8626\n"
     ]
    }
   ],
   "source": [
    "print(f\"## Average on {trials} trials ##\")\n",
    "print(f\"PSNR (imperceptibility): {np.mean(psnrs):.2f} dB\")\n",
    "print(f\"IoU (localization accuracy): {np.mean(ious):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bad2acf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (224) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m save_image(decoded_batch, \u001b[33m\"\u001b[39m\u001b[33meval_localized.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m save_image(\u001b[32m1\u001b[39m-mask, \u001b[33m\"\u001b[39m\u001b[33meval_mask.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m save_image(torch.abs(\u001b[43mimg_edit\u001b[49m\u001b[43m-\u001b[49m\u001b[43moriginal\u001b[49m)*\u001b[32m10\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meval_edit-ori.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m save_image(torch.abs(watermarked-original)*\u001b[32m10\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meval_wm-ori.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (256) must match the size of tensor b (224) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "original = F.interpolate(original, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "watermarked = F.interpolate(watermarked, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "mask = F.interpolate(mask, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "save_image(original, \"eval_original.png\")\n",
    "save_image(watermarked, \"eval_watermarked.png\")\n",
    "save_image(img_edit, \"eval_edited_w_mask.png\")\n",
    "save_image(decoded_batch, \"eval_localized.png\")\n",
    "save_image(1-mask, \"eval_mask.png\")\n",
    "save_image(torch.abs(img_edit-original)*10, \"eval_edit-ori.png\")\n",
    "save_image(torch.abs(watermarked-original)*10, \"eval_wm-ori.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84742381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_logits = torch.cat(logits, dim=0)\n",
    "# # torch.save(total_logits, \"logits_w_l1_loss.pt\")\n",
    "# total_logits = total_logits.cpu().numpy().flatten()\n",
    "# print(f\"Mean: {total_logits.mean():.2f}, Std: {total_logits.std():.2f}, Min: {total_logits.min():.2f}, Max: {total_logits.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab774e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = torch.sigmoid(torch.cat(logits, dim=0)).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c50d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sig, bins=50, alpha=0.7)#, label='A: w/ L1 loss')\n",
    "plt.title('Logit Distribution Comparison')\n",
    "plt.xlabel('Logit Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('logits_comparison.png')\n",
    "# print(\"\\nSaved logit distribution histogram to 'logit_histogram.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logits_a = torch.load(\"logits_wo_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_a = torch.load(\"logits_wo_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_b = torch.load(\"logits_w_l1_loss.pt\").cpu().numpy().flatten()\n",
    "# logits_c = total_logits\n",
    "# print(f\"[A: w/o Add. Loss]Mean: {logits_a.mean():.2f}, Std: {logits_a.std():.2f}, Min: {logits_a.min():.2f}, Max: {logits_a.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss] Mean: {logits_b.mean():.2f}, Std: {logits_b.std():.2f}, Min: {logits_b.min():.2f}, Max: {logits_b.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss (Dual)] Mean: {logits_c.mean():.2f}, Std: {logits_c.std():.2f}, Min: {logits_c.min():.2f}, Max: {logits_c.max():.2f}\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(logits_a, bins=50, alpha=0.4, label='A: w/o L')\n",
    "# plt.hist(logits_b, bins=50, alpha=0.4, label='B: w/ L')\n",
    "# plt.hist(logits_c, bins=50, alpha=0.4, label='B: w/ L (Dual)')\n",
    "# plt.title('Logit Distribution Comparison')\n",
    "# plt.xlabel('Logit Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('logits_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_a = torch.sigmoid(torch.load(\"logits_wo_loss.pt\")).cpu().numpy().flatten()\n",
    "# sig_b = torch.sigmoid(torch.load(\"logits_w_l1_loss.pt\")).cpu().numpy().flatten()\n",
    "# sig_c = torch.sigmoid(torch.load(\"logits_w_l1_loss_dual.pt\")).cpu().numpy().flatten()\n",
    "# print(f\"[A: w/o Add. Loss]Mean: {sig_a.mean():.2f}, Std: {sig_a.std():.2f}, Min: {sig_a.min():.2f}, Max: {sig_a.max():.2f}\")\n",
    "# print(f\"[B: w/ L1 Loss] Mean: {sig_b.mean():.2f}, Std: {sig_b.std():.2f}, Min: {sig_b.min():.2f}, Max: {sig_b.max():.2f}\")\n",
    "# print(f\"[C: w/ L1 Loss (Dual)] Mean: {sig_c.mean():.2f}, Std: {sig_c.std():.2f}, Min: {sig_c.min():.2f}, Max: {sig_c.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d08685",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2196290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(sig_a, bins=50, alpha=0.7, label='A: w/o Add. Loss')\n",
    "# plt.hist(sig_b, bins=50, alpha=0.7, label='B: w/ L1 Loss')\n",
    "# plt.hist(sig_c, bins=50, alpha=0.7, label='B: w/ L1 Loss (Dual)')\n",
    "# plt.title('Logit Distribution Comparison')\n",
    "# plt.xlabel('Logit Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig('logits_comparison.png')\n",
    "# # print(\"\\nSaved logit distribution histogram to 'logit_histogram.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02533add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca09af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stableguard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
